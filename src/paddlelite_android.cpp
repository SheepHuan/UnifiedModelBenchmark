#include "paddle_api.h"
#include <gflags/gflags.h>
#include "math.h"
using namespace paddle::lite_api;


DEFINE_string(model_file, "", "paddlelite model path");
DEFINE_int32(warmup_runs, 3, "warmup_runs");
DEFINE_int32(num_runs, 10, "num runs");
DEFINE_int32(num_threads, 4, "num threads");
DEFINE_int32(cpu_power_mode, 0, "cpu power mode");
DEFINE_bool(use_opencl, false, "use mobile opencl, otherwise use arm cpu");

void run(){

}

int main(int argc, char **argv)
{
    // 解析命令行参数
    gflags::ParseCommandLineFlags(&argc, &argv, true);
    std::string model_file_path = FLAGS_model_file;

    int cpu_power_mode = FLAGS_cpu_power_mode;
    int num_threads = FLAGS_num_threads;
    num_threads = std::min(num_threads,8);
    bool use_opencl = FLAGS_use_opencl; 
   // 1. Set MobileConfig
    MobileConfig config;
    // 2. Set the path to the model generated by opt tools
    config.set_model_from_file(model_file_path);
    if (use_opencl){
        
    }else{
        config.set_threads(4);
        config.set_threads(num_threads);
    }

    // 3. Create PaddlePredictor by MobileConfig
    std::shared_ptr<PaddlePredictor> predictor =
        CreatePaddlePredictor<MobileConfig>(config);

    return 0;
}